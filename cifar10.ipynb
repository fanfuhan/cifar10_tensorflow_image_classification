{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:53:13.702317Z",
     "start_time": "2019-03-22T02:53:13.698327Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cifar10_input\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置算法超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:03.765146Z",
     "start_time": "2019-03-22T02:54:03.760159Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate_init = 0.001\n",
    "l2loss_ratio = 0.001\n",
    "training_epochs = 5\n",
    "batch_size = 100\n",
    "display_step = 100\n",
    "conv1_kernel_num = 64\n",
    "conv2_kernel_num = 64\n",
    "fc1_units_num = 256\n",
    "fc2_units_num = 128\n",
    "fc3_units_num = cifar10_input.NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集中输入图像的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:04.102514Z",
     "start_time": "2019-03-22T02:54:04.098555Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir = './cifar10_data/'\n",
    "image_size = cifar10_input.IMAGE_SIZE\n",
    "image_channel = 3\n",
    "n_classes = cifar10_input.NUM_CLASSES\n",
    "num_examples_per_epoch_for_train = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "num_examples_per_epoch_for_eval = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:04.257248Z",
     "start_time": "2019-03-22T02:54:04.253259Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_distorted_train_batch(data_dir, batch_size):\n",
    "    if not data_dir:\n",
    "        raise ValueError('please supply a data_dir')\n",
    "    data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n",
    "    images, labels = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:04.427409Z",
     "start_time": "2019-03-22T02:54:04.424417Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_undistorted_eval_batch(data_dir, eval_data, batch_size):\n",
    "    if not data_dir:\n",
    "        raise ValueError('please supply a data_dir')\n",
    "    data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n",
    "    images, labels = cifar10_input.inputs(eval_data=eval_data, data_dir=data_dir, batch_size=batch_size)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据指定的维数返回初始化好的指定名称的权重 Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:05.272199Z",
     "start_time": "2019-03-22T02:54:05.268209Z"
    }
   },
   "outputs": [],
   "source": [
    "def WeightsVariable(shape, name_str='weights', stddev=0.1):\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=stddev, dtype=tf.float32)\n",
    "    return tf.Variable(initial, dtype=tf.float32, name=name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据指定的维数返回初始化好的指定名称的权重 Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:05.760783Z",
     "start_time": "2019-03-22T02:54:05.756793Z"
    }
   },
   "outputs": [],
   "source": [
    "def BiasesVariable(shape, name_str='biases', init_value=0.0):\n",
    "    initial = tf.constant(init_value, shape=shape)\n",
    "    return tf.Variable(initial, dtype=tf.float32, name=name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2维卷积层的封装（包含激活函数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:06.235931Z",
     "start_time": "2019-03-22T02:54:06.230944Z"
    }
   },
   "outputs": [],
   "source": [
    "def Conv2d(x, W, b, stride=1, padding='SAME', activation=tf.nn.relu, act_name='relu'):\n",
    "    with tf.name_scope('conv2d_bias'):\n",
    "        y = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "        y = tf.nn.bias_add(y, b)\n",
    "    with tf.name_scope(act_name):\n",
    "        y = activation(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2维池化层pool的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:06.638824Z",
     "start_time": "2019-03-22T02:54:06.634837Z"
    }
   },
   "outputs": [],
   "source": [
    "def Pool2d(x, pool=tf.nn.max_pool, k=2, stride=2, padding='SAME'):\n",
    "    return pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding=padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:06.989776Z",
     "start_time": "2019-03-22T02:54:06.985816Z"
    }
   },
   "outputs": [],
   "source": [
    "def FullyConnected(x, W, b, activation=tf.nn.relu, act_name='relu'):\n",
    "    with tf.name_scope('Wx_b'):\n",
    "        y = tf.matmul(x, W)\n",
    "        y = tf.add(y, b)\n",
    "    with tf.name_scope(act_name):\n",
    "        y = activation(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为每一层的激活输出添加汇总节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:07.319886Z",
     "start_time": "2019-03-22T02:54:07.315863Z"
    }
   },
   "outputs": [],
   "source": [
    "def AddActivationSummary(x):\n",
    "    tf.summary.histogram('/activations', x)\n",
    "    tf.summary.scalar('/sparsity', tf.nn.zero_fraction(x)) # 稀疏性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为所有损失节点添加标量汇总操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:07.872827Z",
     "start_time": "2019-03-22T02:54:07.866843Z"
    }
   },
   "outputs": [],
   "source": [
    "def AddLossesSummary(losses):\n",
    "    # 计算所有损失的滑动平均\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(decay=0.9, name='avg')\n",
    "    loss_averages_op = loss_averages.apply(losses)\n",
    "    \n",
    "    # 为所有损失及平滑处理的损失绑定标量汇总节点\n",
    "    for loss in losses:\n",
    "        tf.summary.scalar(loss.op.name + '(raw)', loss)\n",
    "        tf.summary.scalar(loss.op.name + '(avg)', loss_averages.average(loss))\n",
    "    return loss_averages_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向推断过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:09.499494Z",
     "start_time": "2019-03-22T02:54:09.491517Z"
    }
   },
   "outputs": [],
   "source": [
    "def Inference(images_holder):\n",
    "    # 第一个卷积层\n",
    "    with tf.name_scope('Conv2d_1'):\n",
    "        weights = WeightsVariable(shape=[5, 5, image_channel, conv1_kernel_num], stddev=5e-2)\n",
    "        biases = BiasesVariable(shape=[conv1_kernel_num])\n",
    "        conv1_out = Conv2d(images_holder, weights, biases)\n",
    "        AddActivationSummary(conv1_out)\n",
    "        \n",
    "    # 第一个池化层\n",
    "    with tf.name_scope('Pool2d_1'):\n",
    "        pool1_out = Pool2d(conv1_out, k=3, stride=2)\n",
    "        \n",
    "    # 第二个卷积层\n",
    "    with tf.name_scope('Conv2d_2'):\n",
    "        weights = WeightsVariable(shape=[5, 5, conv1_kernel_num, conv2_kernel_num], stddev=5e-2)\n",
    "        biases = BiasesVariable(shape=[conv2_kernel_num])\n",
    "        conv2_out = Conv2d(pool1_out, weights, biases)\n",
    "        AddActivationSummary(conv2_out)\n",
    "        \n",
    "    # 第二个池化层\n",
    "    with tf.name_scope('Pool2d_2'):\n",
    "        pool2_out = Pool2d(conv2_out, k=3, stride=2)\n",
    "    \n",
    "    # 将二维特征图变为一维特征向量\n",
    "    with tf.name_scope('FeatsReshape'):\n",
    "        features = tf.reshape(pool2_out, [batch_size, -1])\n",
    "        feats_dim = features.get_shape()[1].value  # 得到上一行 -1 所指代的值\n",
    "        \n",
    "    # 第一个全连接层\n",
    "    with tf.name_scope('FC1_nonlinear'):\n",
    "        weights = WeightsVariable(shape=[feats_dim, fc1_units_num], stddev=4e-2)\n",
    "        biases = BiasesVariable(shape=[fc1_units_num], init_value=0.1)\n",
    "        fc1_out = FullyConnected(features, weights, biases)\n",
    "        AddActivationSummary(fc1_out)\n",
    "        # 加入L2损失\n",
    "        with tf.name_scope('L2_loss'):\n",
    "            weight_loss = tf.multiply(tf.nn.l2_loss(weights), l2loss_ratio, name='fc1_weight_loss')\n",
    "            tf.add_to_collection('losses', weight_loss)\n",
    "    \n",
    "    # 第二个全连接层\n",
    "    with tf.name_scope('FC2_nonlinear'):\n",
    "        weights = WeightsVariable(shape=[fc1_units_num, fc2_units_num], stddev=4e-2)\n",
    "        biases = BiasesVariable(shape=[fc2_units_num], init_value=0.1)\n",
    "        fc2_out = FullyConnected(fc1_out, weights, biases)\n",
    "        AddActivationSummary(fc2_out)\n",
    "        # 加入L2损失\n",
    "        with tf.name_scope('L2_loss'):\n",
    "            weight_loss = tf.multiply(tf.nn.l2_loss(weights), l2loss_ratio, name='fc2_weight_loss')\n",
    "            tf.add_to_collection('losses', weight_loss)\n",
    "        \n",
    "    # 第三个全连接层\n",
    "    with tf.name_scope('FC3_linear'):\n",
    "        weights = WeightsVariable(shape=[fc2_units_num, fc3_units_num], stddev=1.0/fc2_units_num)\n",
    "        biases = BiasesVariable(shape=[fc3_units_num])\n",
    "        logits = FullyConnected(fc2_out, weights, biases, activation=tf.identity, act_name='linear')\n",
    "        AddActivationSummary(logits)\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用上面写的函数构造计算图，并设计会话流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:10.203731Z",
     "start_time": "2019-03-22T02:54:10.184784Z"
    }
   },
   "outputs": [],
   "source": [
    "def TrainModel():\n",
    "    with tf.Graph().as_default():\n",
    "        # 计算图输入\n",
    "        with tf.name_scope('Inputs'):\n",
    "            images_holder = tf.placeholder(tf.float32, [batch_size, image_size, image_size, image_channel], name='images')\n",
    "            labels_holder = tf.placeholder(tf.int32, [batch_size], name='labels')\n",
    "\n",
    "        # 计算图前向推断过程\n",
    "        with tf.name_scope('Inference'):\n",
    "            logits = Inference(images_holder)\n",
    "\n",
    "        # 定义损失层\n",
    "        with tf.name_scope('Loss'):\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_holder, logits=logits)\n",
    "            cross_entropy_loss = tf.reduce_mean(cross_entropy, name='xentropy_loss')\n",
    "            tf.add_to_collection('losses', cross_entropy_loss)\n",
    "            # 总损失 = 交叉熵损失 + L2损失\n",
    "            total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "            average_losses = AddLossesSummary(tf.get_collection('losses') + [total_loss])\n",
    "\n",
    "        # 定义优化训练层\n",
    "        with tf.name_scope('Train'):\n",
    "            learning_rate = tf.placeholder(tf.float32)\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int64)\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "            train_op = optimizer.minimize(total_loss, global_step=global_step)\n",
    "\n",
    "        # 定义模型评估层\n",
    "        with tf.name_scope('Evaluate'):\n",
    "            top_K_op = tf.nn.in_top_k(predictions=logits, targets=labels_holder, k=1)\n",
    "\n",
    "        # 定义获取训练样本批次的节点\n",
    "        with tf.name_scope('GetTrainBatch'):\n",
    "            images_train, labels_train = get_distorted_train_batch(data_dir=dataset_dir, batch_size=batch_size)\n",
    "\n",
    "        # 定义获取测试样本批次的节点\n",
    "        with tf.name_scope('GetTestBatch'):\n",
    "            images_test, labels_test = get_undistorted_eval_batch(eval_data=True, data_dir=dataset_dir, batch_size=batch_size)\n",
    "\n",
    "        # 收集所有汇总节点\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "            \n",
    "        # 添加所有变量的初始化节点\n",
    "        init_op = tf.global_variables_initializer()\n",
    "\n",
    "        print(\"把计算图写入事件文件...\")\n",
    "        # graph_writer = tf.summary.FileWriter(logdir='events/', graph=tf.get_default_graph())\n",
    "        # graph_writer.close()\n",
    "        summary_writer = tf.summary.FileWriter(logdir='events/')\n",
    "        summary_writer.add_graph(graph=tf.get_default_graph())\n",
    "        summary_writer.flush()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            \n",
    "            print('==>>>>>>>==开始在训练集上训练模型==<<<<<<<==')\n",
    "            total_batches = int(num_examples_per_epoch_for_train / batch_size)\n",
    "            print(\"per batch size: \", batch_size)\n",
    "            print(\"train sample count per epoch:\", num_examples_per_epoch_for_train)\n",
    "            print(\"total batch count per epoch:\", total_batches)\n",
    "            # 启动数据读取队列\n",
    "            tf.train.start_queue_runners()\n",
    "            # 记录模型被训练的步数\n",
    "            training_step = 0\n",
    "            # 训练指定轮数，每一轮的训练样本总数为：num_examples_per_epoch_for_train\n",
    "            for epoch in range(training_epochs):\n",
    "                # 每一轮都要把所有的batch跑一遍\n",
    "                for batch_idx in range(total_batches):\n",
    "                    # 运行获取批次训练数据的计算图，取出一个批次数据\n",
    "                    images_batch, labels_batch = sess.run([images_train, labels_train])\n",
    "                    # 运行优化器训练节点\n",
    "                    _, loss_value, avg_losses= sess.run([train_op, total_loss, average_losses], feed_dict={images_holder:images_batch, \n",
    "                                                                                labels_holder:labels_batch,\n",
    "                                                                                learning_rate:learning_rate_init})\n",
    "                    # 每调用一次训练节点，training_step就加1，最终 == training_epochs * total_batch\n",
    "                    training_step = sess.run(global_step)\n",
    "                    # 每训练display_step次，计算当前模型的损失和分类准确率\n",
    "                    if training_step % display_step == 0:\n",
    "                        # 运行Evaluate节点，计算当前批次的训练样本的准确率\n",
    "                        predictions = sess.run([top_K_op], feed_dict={images_holder:images_batch, labels_holder:labels_batch})\n",
    "                        # 计算当前批次的预测正确样本量\n",
    "                        batch_accuracy = np.sum(predictions) / batch_size\n",
    "                        print(\"train step: \" + str(training_step) + \", train loss= \" + \"{:.6f}\".format(loss_value) + \n",
    "                              \", train accuracy=\" + \"{:.5f}\".format(batch_accuracy))\n",
    "                        \n",
    "                        # 运行汇总节点\n",
    "                        summaries_str = sess.run(merged_summaries, feed_dict=\n",
    "                                                 {images_holder: images_batch, labels_holder: labels_batch})\n",
    "                        summary_writer.add_summary(summary=summaries_str, global_step=training_step)\n",
    "                        summary_writer.flush()\n",
    "            summary_writer.close()         \n",
    "            print(\"训练完毕！\")\n",
    "                        \n",
    "            print('==>>>>>>>==开始在测试集上评估模型==<<<<<<<==')\n",
    "            total_batches = int(num_examples_per_epoch_for_eval / batch_size)\n",
    "            total_examples = total_batches * batch_size # 当除不尽batch_size时，num_examples_per_epoch_for_evalv ！= total_examples\n",
    "            print(\"per batch size: \", batch_size)\n",
    "            print(\"test sample count per epoch:\", total_examples)\n",
    "            print(\"total batch count per epoch:\", total_batches)\n",
    "            correc_predicted = 0\n",
    "            \n",
    "            for test_step in range(total_batches):\n",
    "                # 运行获取批次测试数据的计算图，取出一个批次数据\n",
    "                images_batch, labels_batch = sess.run([images_test, labels_test])\n",
    "                # 运行Evaluate节点，计算当前批次的训练样本的准确率\n",
    "                predictions = sess.run([top_K_op], feed_dict={images_holder:images_batch, labels_holder:labels_batch})\n",
    "                # 累计每个批次的预测正确样本量\n",
    "                correc_predicted += np.sum(predictions)\n",
    "            \n",
    "            accuracy_score = correc_predicted / total_examples\n",
    "            print(\"-------->accuracy on test examples: \",accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T02:54:10.426007Z",
     "start_time": "2019-03-22T02:54:10.422018Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "    train_dir = './events/'\n",
    "    if tf.gfile.Exists(train_dir):\n",
    "        tf.gfile.DeleteRecursively(train_dir)\n",
    "    tf.gfile.MakeDirs(train_dir)\n",
    "    TrainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:07:13.071034Z",
     "start_time": "2019-03-22T02:54:10.846341Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
